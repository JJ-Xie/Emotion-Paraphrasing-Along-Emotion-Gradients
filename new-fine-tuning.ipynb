{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f664acb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c249ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c9b740",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = input('Dataset to train model (google, msr, quora, mix, twit0.825 or combined):  ')\n",
    "training_type = input('Regular or limited training (regular or limited): ')\n",
    "epochs = int(input('Number of training epochs: '))\n",
    "\n",
    "if training_type.lower() == 'regular':\n",
    "    train_set = 'training'\n",
    "    eval_set = 'testing'\n",
    "elif training_type.lower() == 'limited':\n",
    "    train_set = 'testing'\n",
    "    eval_set = 'training'\n",
    "else:\n",
    "    print('Please enter a valid training type')\n",
    "    \n",
    "emo_filter_list = [\n",
    "                   't5',\n",
    "#                   'bart', \n",
    "#                   'gpt', \n",
    "#                   'nil', \n",
    "#                   'sid',\n",
    "#                   'sid_rg', \n",
    "                   'emo', \n",
    "                   'emo_ge',\n",
    "#                   'emo_nn',\n",
    "#                   'emo_sid', \n",
    "#                   'emo_sid_nn',\n",
    "#                   'emo_sid_tg', \n",
    "#                   'emo_sid_tg_nn', \n",
    "#                   'emo_sid_tg_ge', \n",
    "#                   'emo_sid_tg_nn_ge'\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1af99b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine-tuning T5 by the specified data set, training type, and number of epochs specified above\n",
    "import pandas as pd\n",
    "from simpletransformers.t5 import T5Model\n",
    "\n",
    "def trainModels(emo_filter):\n",
    "    training_df = pd.read_csv(f'emotion-labeled-data/{dataset_name}/{dataset_name}-{emo_filter}-{train_set}_t5.tsv', sep=\"\\t\").astype(str)\n",
    "    testing_df = pd.read_csv(f'emotion-labeled-data/{dataset_name}/{dataset_name}-{emo_filter}-{eval_set}_t5.tsv', sep=\"\\t\").astype(str)\n",
    "    \n",
    "    model_args = {\n",
    "        \"max_seq_length\": 196,\n",
    "        \"train_batch_size\": 6,\n",
    "        \"eval_batch_size\": 32,\n",
    "        \"num_train_epochs\": epochs,\n",
    "        \"evaluate_during_training\": True,\n",
    "        \"evaluate_during_training_steps\": 15000,\n",
    "        \"evaluate_during_training_verbose\": True,\n",
    "\n",
    "        \"use_multiprocessing\": False,\n",
    "        \"fp16\": False,\n",
    "\n",
    "        \"save_steps\": -1,\n",
    "        \"save_eval_checkpoints\": False,\n",
    "        \"save_model_every_epoch\": False,\n",
    "\n",
    "        \"reprocess_input_data\": True,\n",
    "        \"overwrite_output_dir\": True,\n",
    "\n",
    "        \"output_dir\": f\"{training_type}-{dataset_name}-{emo_filter}-{epochs}epochs\", \n",
    "\n",
    "    }\n",
    "\n",
    "    model = T5Model(\"t5\", \"t5-base\", args=model_args)\n",
    "    model.train_model(training_df, eval_data=testing_df)\n",
    "    \n",
    "    \n",
    "for emo_filter in emo_filter_list:\n",
    "    print(\"Training on \" + emo_filter)\n",
    "    if (emo_filter == 't5'):\n",
    "        print('T5_Base Pretrained.')\n",
    "    else: \n",
    "        trainModels(emo_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d21774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeaa25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning BART by the specified data set, training type, and number of epochs specified above\n",
    "import pandas as pd\n",
    "from simpletransformers.seq2seq import Seq2SeqModel, Seq2SeqArgs\n",
    "\n",
    "def trainModels(emo_filter):\n",
    "    training_df = pd.read_csv(f'emotion-labeled-data/{dataset_name}/{dataset_name}-{emo_filter}-{train_set}_t5.tsv', sep=\"\\t\").astype(str)\n",
    "    testing_df = pd.read_csv(f'emotion-labeled-data/{dataset_name}/{dataset_name}-{emo_filter}-{eval_set}_t5.tsv', sep=\"\\t\").astype(str)\n",
    "\n",
    "    # General parameters for fine-tuning\n",
    "    model_args = {\n",
    "        \"max_seq_length\": 196,\n",
    "        \"train_batch_size\": 4,\n",
    "        \"eval_batch_size\": 32,\n",
    "        \"num_train_epochs\": epochs,\n",
    "        \"evaluate_during_training\": True,\n",
    "        \"evaluate_during_training_steps\": 15000,\n",
    "        \"evaluate_during_training_verbose\": True,\n",
    "    \n",
    "        \"use_multiprocessing\": False,\n",
    "        \"fp16\": False,\n",
    "\n",
    "        \"save_steps\": -1,\n",
    "        \"save_eval_checkpoints\": False,\n",
    "        \"save_model_every_epoch\": False,\n",
    "\n",
    "        \"reprocess_input_data\": True,\n",
    "        \"overwrite_output_dir\": True,\n",
    "    \n",
    "        # Creates special directory to hold each new model\n",
    "        \"output_dir\": f\"{training_type}-{dataset_name}-{emo_filter}-{epochs}epochs-bart\", \n",
    "    }\n",
    "\n",
    "\n",
    "    # Use bart-large as the model to be fine-tuned\n",
    "    model = Seq2SeqModel(\n",
    "        encoder_decoder_type=\"bart\",\n",
    "        encoder_decoder_name=\"facebook/bart-base\",\n",
    "        args=model_args,\n",
    "    )\n",
    "\n",
    "    # Fine-tuning the bart model\n",
    "    model.train_model(training_df, eval_data=testing_df)\n",
    "    \n",
    "for emo_filter in emo_filter_list:\n",
    "    if (emo_filter == 'bart'):\n",
    "        print('Bart_Base Pretrained.')\n",
    "    else: \n",
    "        print('Training Bart Base on ' + emo_filter)\n",
    "        trainModels(emo_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dca859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9068b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fine-tuning GPT2 by the specified data set, training type, and number of epochs specified above\n",
    "import pandas as pd\n",
    "#from simpletransformers.language_modeling import LanguageModelingModel, LanguageModelingArgs\n",
    "from transformers import (\n",
    "    AutoModelWithLMHead,\n",
    "    AutoConfig,\n",
    "    Trainer,\n",
    "    AutoTokenizer,\n",
    "    TextDataset,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "def convertGPTinput(emo_filter):\n",
    "    df = pd.read_csv(f'emotion-labeled-data/{dataset_name}/{dataset_name}-{emo_filter}-training_t5.tsv', encoding='utf-8', sep='\\t')[['prefix', 'input_text', 'target_text']]\n",
    "    df['combined'] = '<s>' + df.prefix + ': ' + df.input_text + '</s>'+ '>>>>' + '<p>' + df.target_text + '</p>'\n",
    "    df['combined'] = df.combined.to_csv(f'emotion-labeled-data/{dataset_name}/{dataset_name}-{emo_filter}-training_gpt.txt', sep='\\n', index = False)\n",
    "\n",
    "def trainGPT(emo_filter, text_path, epochs, model='gpt2', batch_size=6, cache_dir='cache'):\n",
    "    model = AutoModelWithLMHead.from_pretrained(model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    train_dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path = text_path,\n",
    "        block_size = 256\n",
    "    )\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = f\"{training_type}-{dataset_name}-{emo_filter}-{epochs}epochs-gpt\",\n",
    "        num_train_epochs = epochs,\n",
    "        per_device_train_batch_size = batch_size,\n",
    "        warmup_steps = 500,\n",
    "        save_steps = 2000,\n",
    "        logging_steps = 500\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        data_collator = data_collator,\n",
    "        train_dataset = train_dataset\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "\n",
    "        \n",
    "for emo_filter in emo_filter_list:\n",
    "    if (emo_filter == 'gpt'):\n",
    "        print('GPT2 Pretrained.')\n",
    "    else: \n",
    "        print('Training GPT2 on ' + emo_filter)\n",
    "        convertGPTinput(emo_filter)\n",
    "        trainGPT(\n",
    "            emo_filter=emo_filter,\n",
    "            text_path=f'emotion-labeled-data/{dataset_name}/{dataset_name}-{emo_filter}-training_gpt.txt',\n",
    "            epochs=3\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49c6813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
