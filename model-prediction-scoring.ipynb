{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cf606e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers==2.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e39b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e533c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import os\n",
    "from transformers import BertTokenizer\n",
    "from GoEmotions.model import BertForMultiLabelClassification\n",
    "from GoEmotions.multilabel_pipeline import MultiLabelPipeline\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"monologg/bert-base-cased-goemotions-original\")\n",
    "model = BertForMultiLabelClassification.from_pretrained(\"monologg/bert-base-cased-goemotions-original\")\n",
    "\n",
    "goemotions = MultiLabelPipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    threshold=0.3,\n",
    "    device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf053d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dataset = input('Dataset used to train model: ')\n",
    "training_type = input('Training type of model: ')\n",
    "epochs = int(input('Number of epochs trained on model'))\n",
    "model_name = f\"{training_type}-{model_dataset}-{epochs}epochs\"\n",
    "\n",
    "testing_set = input('Dataset for prediction generation: ')\n",
    "\n",
    "if training_type.lower() == 'regular':\n",
    "    train_set = 'training'\n",
    "    eval_set = 'testing'\n",
    "elif training_type.lower() == 'limited':\n",
    "    train_set = 'testing'\n",
    "    eval_set = 'training'\n",
    "else:\n",
    "    print('Please enter a valid training type')\n",
    "\n",
    "def print_base_info(model_name, testing_set):\n",
    "    print(f'---- Scoring Predictions ----')\n",
    "    print(f'Model: {model_name}')\n",
    "    print(f'Test Set: {testing_set}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960be6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "def Top_Score_Label (outputs):\n",
    "    scores = 1 / (1 + np.exp(-outputs))  # Sigmoid\n",
    "    top_score = 0\n",
    "    top_label = \"\"\n",
    "    for item in scores:\n",
    "        for idx, s in enumerate(item):\n",
    "            if s > threshold:\n",
    "                if s > top_score: \n",
    "                    top_label = model.config.id2label[idx]\n",
    "    return top_label\n",
    "\n",
    "print(f'---- Labeling Predictions for Model: {model_name} on Dataset: {testing_set} ----')\n",
    "\n",
    "df = pd.read_csv(f'predictions-data/{model_name}/{testing_set}-predictions.tsv', sep='\\t').astype(str) \n",
    "\n",
    "target_labels = []\n",
    "prediction_labels = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    t_text = (row.target_text[:512] + '..') if len(row.target_text) > 512 else row.target_text\n",
    "    p_text = (row.predictions[:512] + '..') if len(row.predictions) > 512 else row.predictions\n",
    "\n",
    "    target_emo = goemotions(t_text)\n",
    "    prediction_emo = goemotions(p_text)\n",
    "\n",
    "    target_label = Top_Score_Label(target_emo)\n",
    "    prediction_label = Top_Score_Label(prediction_emo)\n",
    "\n",
    "    target_labels.append(target_label)\n",
    "    prediction_labels.append(prediction_label)\n",
    "\n",
    "df[\"target_emo\"] = target_labels \n",
    "df[\"prediction_emo\"] = prediction_labels\n",
    "\n",
    "df.to_csv(f'predictions-data/{model_name}/{testing_set}-prediction_emo.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3695805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import evaluate\n",
    "\n",
    "from pprint import pprint\n",
    "from statistics import mean\n",
    "\n",
    "def exact(truths, preds):\n",
    "    exact = evaluate.load('exact_match')\n",
    "    result = exact.compute(predictions = preds, references = truths)['exact_match']\n",
    "    return result\n",
    "\n",
    "def bleu(truths, preds):\n",
    "    bleu = evaluate.load('bleu')\n",
    "    result = bleu.compute(predictions = preds, references = truths)['bleu']\n",
    "    return result\n",
    "\n",
    "def google_bleu(truths, preds):\n",
    "    google_bleu = evaluate.load('google_bleu')\n",
    "    result = google_bleu.compute(predictions = preds, references = truths)['google_bleu']\n",
    "    return result\n",
    "\n",
    "def rouge1(truths, preds):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    result = rouge.compute(predictions = preds, references = truths)['rouge1']\n",
    "    return result\n",
    "    \n",
    "def rouge2(truths, preds):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    result = rouge.compute(predictions = preds, references = truths)['rouge2']\n",
    "    return result\n",
    "    \n",
    "def rougeL(truths, preds):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    result = rouge.compute(predictions = preds, references = truths)['rougeL']\n",
    "    return result\n",
    "\n",
    "def bertscore(truths, preds):\n",
    "    bscore = evaluate.load('bertscore')\n",
    "    result = bscore.compute(predictions = preds, references = truths, model_type=\"distilbert-base-uncased\")\n",
    "    return result\n",
    "\n",
    "def meteor(truths, preds):\n",
    "    meteor = evaluate.load('meteor')\n",
    "    result = meteor.compute(predictions = preds, references = truths)['meteor']\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print_base_info(model_name, testing_set)\n",
    "    \n",
    "df = pd.read_csv(f'predictions-data/{model_name}/{testing_set}-prediction_emo.tsv', sep='\\t').astype(str)\n",
    "\n",
    "pprint(\"Exact Score\")\n",
    "pprint(exact(df[\"target_emo\"], df[\"prediction_emo\"]))\n",
    "\n",
    "pprint(\"BLEU Score\")\n",
    "pprint(bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "pprint('Google BLEU Score')\n",
    "pprint(google_bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "pprint('ROUGE1 Score')\n",
    "pprint(rouge1(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "pprint('ROUGE2 Score')\n",
    "pprint(rouge2(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "pprint('ROUGEL Score')\n",
    "pprint(rougeL(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "pprint('METEOR Score')\n",
    "pprint(meteor(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2df37fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
