{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6002bda5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers==2.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec099ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a39611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import os\n",
    "from transformers import BertTokenizer\n",
    "from GoEmotions.model import BertForMultiLabelClassification\n",
    "from GoEmotions.multilabel_pipeline import MultiLabelPipeline\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"monologg/bert-base-cased-goemotions-original\")\n",
    "model = BertForMultiLabelClassification.from_pretrained(\"monologg/bert-base-cased-goemotions-original\")\n",
    "\n",
    "goemotions = MultiLabelPipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    threshold=0.3,\n",
    "    device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43224a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dataset = input('Dataset to train model (google, msr, quora, mix, twit0.825 or combined):  ')\n",
    "training_type = input('Regular or limited training (regular or limited): ')\n",
    "epochs = int(input('Number of training epochs: '))\n",
    "\n",
    "testing_set = input('Dataset for prediction generation (google, msr, quora, mix, twit0.825, or combined): ')\n",
    "\n",
    "if training_type.lower() == 'regular':\n",
    "    train_set = 'training'\n",
    "    eval_set = 'testing'\n",
    "elif training_type.lower() == 'limited':\n",
    "    train_set = 'testing'\n",
    "    eval_set = 'training'\n",
    "else:\n",
    "    print('Please enter a valid training type')\n",
    "\n",
    "def print_base_info(model_name, testing_set, eval_emo_filter):\n",
    "    print(f'---- Scoring Predictions ----')\n",
    "    print(f'Model: {model_name}')\n",
    "    print(f'Test Set: {testing_set}-{eval_emo_filter}')\n",
    "    \n",
    "emo_filter_list = [\n",
    "                   't5',\n",
    "#                   'bart',\n",
    "#                   'gpt', \n",
    "#                   'nil', \n",
    "#                   'sid',\n",
    "#                   'sid_rg', \n",
    "                   'emo', \n",
    "                   'emo_ge',\n",
    "#                   'emo_nn',\n",
    "#                   'emo_sid', \n",
    "#                   'emo_sid_nn',\n",
    "#                   'emo_sid_tg', \n",
    "#                   'emo_sid_tg_nn', \n",
    "#                   'emo_sid_tg_ge', \n",
    "#                   'emo_sid_tg_nn_ge'\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f169ba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_neg_emo = {'anger', 'disgust', 'grief', 'fear', 'sadness'}\n",
    "low_neg_emo = {'nervousness', 'annoyance', 'disappointment', 'embarrassment', 'remorse', 'disapproval'}\n",
    "neutral_emo = {'confusion', 'curiosity', 'realization', 'surprise', 'neutral'}\n",
    "low_pos_emo = {'approval', 'caring', 'desire', 'relief'}\n",
    "high_pos_emo = {'amusement', 'excitement', 'pride', 'optimism', 'gratitude', 'joy', 'admiration', 'love'}\n",
    "\n",
    "high_neg_threshold = -0.4\n",
    "high_pos_threshold = 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae76e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels the target and predicted texts for scoring by emotion transition and paraphrasing metrics with T5\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Same Sigmoid Function for determining the emotion of a text\n",
    "threshold = 0.5\n",
    "def Top_Score_Label (outputs):\n",
    "    scores = 1 / (1 + np.exp(-outputs))  # Sigmoid\n",
    "    top_score = 0\n",
    "    top_label = \"\"\n",
    "    for item in scores:\n",
    "        for idx, s in enumerate(item):\n",
    "            if s > threshold:\n",
    "                if s > top_score: \n",
    "                    top_label = model.config.id2label[idx]\n",
    "    return top_label\n",
    "\n",
    "def labelSid(model_name, eval_emo_filter, df):\n",
    "    prediction_labels = []\n",
    "    for index, row in df.iterrows():\n",
    "        prediction_sid = sid.polarity_scores(rowcalculate_emos_trans.predictions)['compound']\n",
    "\n",
    "        prediction_labels.append(prediction_sid)\n",
    "   \n",
    "    df[\"prediction_emo\"] = prediction_labels\n",
    "\n",
    "    df.to_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t')\n",
    "    \n",
    "\n",
    "def labelSidRg(model_name, eval_emo_filter, df):\n",
    "    prediction_labels = []\n",
    "    for index, row in df.iterrows():\n",
    "        prediction_sid = sid.polarity_scores(row.predictions)['compound']\n",
    "\n",
    "        if prediction_sid <= high_neg_threshold:\n",
    "            prediction_sid_range = 'high_neg'\n",
    "        elif prediction_sid > high_neg_threshold and prediction_sid < 0:\n",
    "            prediction_sid_range = 'low_neg'\n",
    "        elif prediction_sid == 0:\n",
    "            prediction_sid_range = 'neutral'\n",
    "        elif prediction_sid < high_pos_threshold and prediction_sid > 0:\n",
    "            prediction_sid_range = 'low_pos'\n",
    "        elif prediction_sid >= high_pos_threshold:\n",
    "            prediction_sid_range = 'high_pos'\n",
    "\n",
    "        prediction_labels.append(prediction_sid_range)\n",
    "   \n",
    "    df[\"prediction_emo\"] = prediction_labels\n",
    "\n",
    "    df.to_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t')\n",
    "\n",
    "    \n",
    "def labelEmo(model_name, eval_emo_filter, df):\n",
    "    target_labels = [] \n",
    "    prediction_labels = []\n",
    "    prediction_labels_ge = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        t_text = (row.target_text[:512] + '..') if len(row.target_text) > 512 else row.target_text\n",
    "        target_emo = goemotions(t_text)\n",
    "        target_label = Top_Score_Label(target_emo)\n",
    "        target_labels.append(target_label)\n",
    "        \n",
    "        p_text = (row.predictions[:512] + '..') if len(row.predictions) > 512 else row.predictions\n",
    "\n",
    "        prediction_emo = goemotions(p_text)\n",
    "\n",
    "        prediction_label = Top_Score_Label(prediction_emo)\n",
    "\n",
    "        prediction_emo_range = 'neutral'\n",
    "        if prediction_label in high_neg_emo:\n",
    "            prediction_emo_range = 'high_neg'\n",
    "        elif prediction_label in low_neg_emo:\n",
    "            prediction_emo_range = 'low_neg'\n",
    "        elif prediction_label in neutral_emo:\n",
    "            prediction_emo_range = 'neutral'\n",
    "        elif prediction_label in low_pos_emo:\n",
    "            prediction_emo_range = 'low_pos'\n",
    "        elif prediction_label in high_pos_emo:\n",
    "            prediction_emo_range = 'high_pos'\n",
    "\n",
    "        prediction_labels_ge.append(prediction_label)\n",
    "        prediction_labels.append(prediction_emo_range)\n",
    "\n",
    "    df[\"target_emo_ge\"] = target_labels\n",
    "    df[\"prediction_emo_ge\"] = prediction_labels_ge\n",
    "    df[\"prediction_emo\"] = prediction_labels\n",
    "\n",
    "    df.to_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t')\n",
    "\n",
    "    \n",
    "def labelGoEmo(model_name, eval_emo_filter, df):\n",
    "    target_labels = [] \n",
    "    prediction_labels = []\n",
    "    prediction_labels_ge = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        target_label = row.target_emo\n",
    "        if target_label in high_neg_emo:\n",
    "            target_emo_range = 'high_neg'\n",
    "        elif target_label in low_neg_emo:\n",
    "            target_emo_range = 'low_neg'\n",
    "        elif target_label in neutral_emo:\n",
    "            target_emo_range = 'neutral'\n",
    "        elif target_label in low_pos_emo:\n",
    "            target_emo_range = 'low_pos'\n",
    "        elif target_label in high_pos_emo:\n",
    "            target_emo_range = 'high_pos'\n",
    "\n",
    "        target_labels.append(target_emo_range)\n",
    "\n",
    "        p_text = (row.predictions[:512] + '..') if len(row.predictions) > 512 else row.predictions\n",
    "\n",
    "        prediction_emo = goemotions(p_text)\n",
    "\n",
    "        prediction_label = Top_Score_Label(prediction_emo)\n",
    "        \n",
    "        prediction_emo_range = 'neutral'\n",
    "        if prediction_label in high_neg_emo:\n",
    "            prediction_emo_range = 'high_neg'\n",
    "        elif prediction_label in low_neg_emo:\n",
    "            prediction_emo_range = 'low_neg'\n",
    "        elif prediction_label in neutral_emo:\n",
    "            prediction_emo_range = 'neutral'\n",
    "        elif prediction_label in low_pos_emo:\n",
    "            prediction_emo_range = 'low_pos'\n",
    "        elif prediction_label in high_pos_emo:\n",
    "            prediction_emo_range = 'high_pos'\n",
    "\n",
    "        prediction_labels_ge.append(prediction_label)\n",
    "        prediction_labels.append(prediction_emo_range)\n",
    "\n",
    "    df = df.rename(columns={\"target_emo\": \"target_emo_ge\"})\n",
    "    df[\"target_emo\"] = target_labels\n",
    "    df[\"prediction_emo_ge\"] = prediction_labels_ge\n",
    "    df[\"prediction_emo\"] = prediction_labels\n",
    "\n",
    "    df.to_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t')\n",
    "\n",
    "    \n",
    "def labelPredictions(model_emo_filter, eval_emo_filter):\n",
    "    if (model_emo_filter == 't5'): \n",
    "        model_name = 't5-base'\n",
    "    else:\n",
    "        model_name = f\"{training_type}-{model_dataset}-{model_emo_filter}-{epochs}epochs\"\n",
    "    print(f'---- Labeling Predictions for Model: {model_name} on Dataset: {testing_set}-{eval_emo_filter} ----')\n",
    "\n",
    "    df = pd.read_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-predictions.tsv', sep='\\t').astype(str) \n",
    "        \n",
    "    if model_emo_filter == 'sid':\n",
    "        labelSid(model_name, eval_emo_filter, df)\n",
    "    elif model_emo_filter == 'sid_rg':\n",
    "        labelSidRg(model_name, eval_emo_filter, df)\n",
    "    elif '_ge' in model_emo_filter or '_ge' in eval_emo_filter or model_emo_filter == 'nil' or eval_emo_filter == 'nil': \n",
    "        labelGoEmo(model_name, eval_emo_filter, df)\n",
    "    else:\n",
    "        labelEmo(model_name, eval_emo_filter, df)\n",
    "        \n",
    "for emo_filter in emo_filter_list:\n",
    "    if (emo_filter == 't5'): \n",
    "#        labelPredictions('t5', 'nil')     \n",
    "        labelPredictions('t5', 'emo')\n",
    "        labelPredictions('t5', 'emo_ge')\n",
    "#        labelPredictions('nil', 'emo_ge')\n",
    "    else:\n",
    "        labelPredictions(emo_filter, emo_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75568c6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import evaluate\n",
    "\n",
    "from pprint import pprint\n",
    "from statistics import mean\n",
    "\n",
    "# Exact Match scores emotion transition\n",
    "def exact(truths, preds):\n",
    "    exact = evaluate.load('exact_match')\n",
    "    result = exact.compute(predictions = preds, references = truths)['exact_match']\n",
    "    return result\n",
    "\n",
    "# BLEU, Google_BLEU, ROUGE, and METEOR score paraphrasing\n",
    "def bleu(truths, preds):\n",
    "    bleu = evaluate.load('bleu')\n",
    "    result = bleu.compute(predictions = preds, references = truths)['bleu']\n",
    "    return result\n",
    "\n",
    "def google_bleu(truths, preds):\n",
    "    google_bleu = evaluate.load('google_bleu')\n",
    "    result = google_bleu.compute(predictions = preds, references = truths)['google_bleu']\n",
    "    return result\n",
    "\n",
    "def rouge1(truths, preds):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    result = rouge.compute(predictions = preds, references = truths)['rouge1']\n",
    "    return result\n",
    "    \n",
    "def rouge2(truths, preds):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    result = rouge.compute(predictions = preds, references = truths)['rouge2']\n",
    "    return result\n",
    "    \n",
    "def rougeL(truths, preds):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    result = rouge.compute(predictions = preds, references = truths)['rougeL']\n",
    "    return result\n",
    "\n",
    "def bertscore(truths, preds):\n",
    "    bscore = evaluate.load('bertscore')\n",
    "    result = bscore.compute(predictions = preds, references = truths, model_type=\"distilbert-base-uncased\")\n",
    "    return result\n",
    "\n",
    "def meteor(truths, preds):\n",
    "    meteor = evaluate.load('meteor')\n",
    "    result = meteor.compute(predictions = preds, references = truths)['meteor']\n",
    "    return result\n",
    "\n",
    "\n",
    "def scoreSid(model_emo_filter, eval_emo_filter):\n",
    "    if (model_emo_filter == 'T5'): \n",
    "        model_name = 't5-base'\n",
    "    else:\n",
    "        model_name = f\"{training_type}-{model_dataset}-{model_emo_filter}-{epochs}epochs\"\n",
    "    print_base_info(model_name, testing_set, eval_emo_filter)\n",
    "    df = pd.read_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t').astype(str)\n",
    "    \n",
    "    row_count = 0\n",
    "    lowered_intensity = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        row_count += 1\n",
    "        if float(row.input_emo) > float(row.prediction_emo):\n",
    "            lowered_intensity += 1\n",
    "    \n",
    "    pprint(\"Exact Score\")\n",
    "    pprint(lowered_intensity/row_count)\n",
    "\n",
    "    pprint(\"BLEU Score\")\n",
    "    pprint(bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('Google BLEU Score')\n",
    "    pprint(google_bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('ROUGE1 Score')\n",
    "    pprint(rouge1(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('ROUGE2 Score')\n",
    "    pprint(rouge2(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('ROUGEL Score')\n",
    "    pprint(rougeL(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('METEOR Score')\n",
    "    pprint(meteor(df[\"target_text\"], df[\"predictions\"]))\n",
    "    \n",
    "#    pprint('BERTScore')\n",
    "#    pprint(bertscore(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "def scorePredictions(model_emo_filter, eval_emo_filter):\n",
    "    if (model_emo_filter == 't5'): \n",
    "        model_name = 't5-base'\n",
    "    else:\n",
    "        model_name = f\"{training_type}-{model_dataset}-{model_emo_filter}-{epochs}epochs\"\n",
    "    print_base_info(model_name, testing_set, eval_emo_filter)\n",
    "    df = pd.read_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t').astype(str)\n",
    "    scores = {'Model Name & Test Set': model_name + ' & ' + f'{testing_set}-{eval_emo_filter}'}\n",
    "#    scores.update({'Test Set': f'{testing_set}-{eval_emo_filter}'})\n",
    "\n",
    "#    pprint(\"Exact Score\")\n",
    "#    pprint(exact(df[\"target_emo\"], df[\"prediction_emo\"]))\n",
    "    scores.update({'Exact': exact(df[\"target_emo\"], df[\"prediction_emo\"])})\n",
    "\n",
    "#    if ('target_emo_ge' in df.columns) and ('prediction_emo_ge' in df.columns): \n",
    "#        pprint(\"Exact Score GE\")\n",
    "#        pprint(exact(df[\"target_emo_ge\"], df[\"prediction_emo_ge\"]))\n",
    "    scores.update({'Exact GE': exact(df[\"target_emo_ge\"], df[\"prediction_emo_ge\"])})\n",
    "\n",
    "#    pprint(\"BLEU Score\")\n",
    "#    pprint(bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "    scores.update({'BLEU': bleu(df[\"target_text\"], df[\"predictions\"])})\n",
    "\n",
    "#    pprint('Google BLEU Score')\n",
    "#    pprint(google_bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "    scores.update({'Google BLEU': google_bleu(df[\"target_text\"], df[\"predictions\"])})\n",
    "\n",
    "#    pprint('ROUGE1 Score')\n",
    "#    pprint(rouge1(df[\"target_text\"], df[\"predictions\"]))\n",
    "    scores.update({'ROUGE1': rouge1(df[\"target_text\"], df[\"predictions\"])})\n",
    "\n",
    "#    pprint('ROUGE2 Score')\n",
    "#    pprint(rouge2(df[\"target_text\"], df[\"predictions\"]))\n",
    "    scores.update({'ROUGE2': rouge2(df[\"target_text\"], df[\"predictions\"])})\n",
    "\n",
    "#    pprint('ROUGEL Score')\n",
    "#    pprint(rougeL(df[\"target_text\"], df[\"predictions\"]))\n",
    "    scores.update({'ROUGEL': rougeL(df[\"target_text\"], df[\"predictions\"])})\n",
    "\n",
    "#    pprint('METEOR Score')\n",
    "#    pprint(meteor(df[\"target_text\"], df[\"predictions\"]))\n",
    "    scores.update({'METEOR': meteor(df[\"target_text\"], df[\"predictions\"])})\n",
    "\n",
    "#    pprint('BERTScore')\n",
    "#    pprint(bertscore(list(df[\"target_text\"]), list(df[\"predictions\"])))\n",
    "#    score.update({'BERTScore': bertscore(df[\"target_text\"], df[\"predictions\"])})\n",
    "\n",
    "    range_trans_counts = {}\n",
    "    range_trans_hits = {}\n",
    "    for index, row in df.iterrows():\n",
    "        if '_ge' in model_emo_filter or '_ge' in eval_emo_filter or model_emo_filter == 'nil' or eval_emo_filter == 'nil':\n",
    "            if row.input_emo in high_neg_emo:\n",
    "                input_emo_range = 'high_neg'\n",
    "            elif row.input_emo in low_neg_emo:\n",
    "                input_emo_range = 'low_neg'\n",
    "            elif row.input_emo in neutral_emo:\n",
    "                input_emo_range = 'neutral'\n",
    "            elif row.input_emo in low_pos_emo:\n",
    "                input_emo_range = 'low_pos'\n",
    "            elif row.input_emo in high_pos_emo:\n",
    "                input_emo_range = 'high_pos'\n",
    "            target_emo_range = row.target_emo\n",
    "        else: \n",
    "            input_emo_range = row.input_emo\n",
    "            target_emo_range = row.target_emo\n",
    "\n",
    "\n",
    "        key = input_emo_range + \" to \" + target_emo_range\n",
    "        if key in range_trans_counts.keys(): \n",
    "            range_trans_counts.update({key: range_trans_counts[key]+1})\n",
    "            if target_emo_range == row.prediction_emo: \n",
    "                if key in range_trans_hits.keys():\n",
    "                    range_trans_hits.update({key: range_trans_hits[key]+1}) \n",
    "                else:\n",
    "                    range_trans_hits.update({key: 1}) \n",
    "        else:\n",
    "            range_trans_counts.update({key: 1})\n",
    "            if target_emo_range == row.prediction_emo: \n",
    "                if key in range_trans_hits.keys():\n",
    "                    range_trans_hits.update({key: range_trans_hits[key]+1}) \n",
    "                else:\n",
    "                    range_trans_hits.update({key: 1}) \n",
    "\n",
    "                \n",
    "    for key, value in range_trans_counts.items(): \n",
    "        if (key in range_trans_hits): \n",
    "            hits = range_trans_hits[key]\n",
    "        else: \n",
    "            hits = 0 \n",
    "#        print(key + ': ', str(hits/value))\n",
    "        scores.update({key: hits/value})\n",
    "\n",
    "    print('\\n')\n",
    "            \n",
    "    for key, value in scores.items(): \n",
    "        print(key + ': ', str(value))\n",
    "    print('\\n')\n",
    "\n",
    "    return scores\n",
    "\n",
    "scores_df = pd.DataFrame()\n",
    "\n",
    "for emo_filter in emo_filter_list:\n",
    "    if emo_filter == 'sid':\n",
    "        scoreSid(emo_filter, emo_filter)\n",
    "    else:\n",
    "        if (emo_filter == 't5'): \n",
    "#            scores = scorePredictions('t5', 'nil')\n",
    "#            scores_df = scores_df.append(scores, ignore_index=True)\n",
    "            scores = scorePredictions('t5', 'emo')\n",
    "            scores_df = scores_df.append(scores, ignore_index=True)\n",
    "            scores = scorePredictions('t5', 'emo_ge')\n",
    "            scores_df = scores_df.append(scores, ignore_index=True)\n",
    "#            scores = scorePredictions('nil', 'emo_ge')\n",
    "#            scores_df = scores_df.append(scores, ignore_index=True)\n",
    "        else: \n",
    "            scores = scorePredictions(emo_filter, emo_filter)\n",
    "            scores_df = scores_df.append(scores, ignore_index=True)\n",
    "\n",
    "print(scores_df)\n",
    "scores_df.to_csv(f'predictions-data/{model_dataset}-{training_type}-{epochs}epochs-all-scores.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9681dfe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c410d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelPredictions('emo', 'emo_sid_tg_nn')\n",
    "scorePredictions('emo', 'emo_sid_tg_nn')\n",
    "\n",
    "labelPredictions('emo_ge', 'emo_sid_tg_nn_ge')\n",
    "scorePredictions('emo_ge', 'emo_sid_tg_nn_ge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cd1350",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelPredictions('emo', 'emo') \n",
    "scorePredictions('emo', 'emo') \n",
    "\n",
    "labelPredictions('t5', 'emo') \n",
    "scorePredictions('t5', 'emo') \n",
    "\n",
    "labelPredictions('emo_ge', 'emo_ge') \n",
    "scorePredictions('emo_ge', 'emo_ge') \n",
    "\n",
    "labelPredictions('t5', 'emo_ge') \n",
    "scorePredictions('t5', 'emo_ge') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b84ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelPredictions('t5', 'emo_sid_tg_nn_ge') \n",
    "scorePredictions('t5', 'emo_sid_tg_nn_ge') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b54ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelPredictions('emo', 'emo_sid_tg_nn') \n",
    "labelPredictions('emo_nn', 'emo_sid_tg_nn') \n",
    "labelPredictions('emo_sid', 'emo_sid_tg_nn') \n",
    "labelPredictions('emo_sid_nn', 'emo_sid_tg_nn') \n",
    "labelPredictions('emo_sid_tg', 'emo_sid_tg_nn') \n",
    "labelPredictions('emo_sid_tg_nn', 'emo_sid_tg_nn') \n",
    "labelPredictions('emo_sid_tg_ge', 'emo_sid_tg_nn_ge') \n",
    "labelPredictions('emo_sid_tg_nn_ge', 'emo_sid_tg_nn_ge') \n",
    "\n",
    "scorePredictions('emo', 'emo_sid_tg_nn') \n",
    "scorePredictions('emo_nn', 'emo_sid_tg_nn') \n",
    "scorePredictions('emo_sid', 'emo_sid_tg_nn') \n",
    "scorePredictions('emo_sid_nn', 'emo_sid_tg_nn') \n",
    "scorePredictions('emo_sid_tg', 'emo_sid_tg_nn') \n",
    "scorePredictions('emo_sid_tg_nn', 'emo_sid_tg_nn') \n",
    "scorePredictions('emo_sid_tg_ge', 'emo_sid_tg_nn_ge') \n",
    "scorePredictions('emo_sid_tg_nn_ge', 'emo_sid_tg_nn_ge') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6286c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels the target and predicted texts for scoring by emotion transition and paraphrasing metrics with Bart\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Same Sigmoid Function for determining the emotion of a text\n",
    "threshold = 0.5\n",
    "def Top_Score_Label (outputs):\n",
    "    scores = 1 / (1 + np.exp(-outputs))  # Sigmoid\n",
    "    top_score = 0\n",
    "    top_label = \"\"\n",
    "    for item in scores:\n",
    "        for idx, s in enumerate(item):\n",
    "            if s > threshold:\n",
    "                if s > top_score: \n",
    "                    top_label = model.config.id2label[idx]\n",
    "    return top_label\n",
    "\n",
    "def labelSid(model_name, eval_emo_filter, df):\n",
    "    prediction_labels = []\n",
    "    for index, row in df.iterrows():\n",
    "        prediction_sid = sid.polarity_scores(rowcalculate_emos_trans.predictions)['compound']\n",
    "\n",
    "        prediction_labels.append(prediction_sid)\n",
    "   \n",
    "    df[\"prediction_emo\"] = prediction_labels\n",
    "\n",
    "    df.to_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t')\n",
    "    \n",
    "\n",
    "def labelSidRg(model_name, eval_emo_filter, df):\n",
    "    prediction_labels = []\n",
    "    for index, row in df.iterrows():\n",
    "        prediction_sid = sid.polarity_scores(row.predictions)['compound']\n",
    "\n",
    "        if prediction_sid <= high_neg_threshold:\n",
    "            prediction_sid_range = 'high_neg'\n",
    "        elif prediction_sid > high_neg_threshold and prediction_sid < 0:\n",
    "            prediction_sid_range = 'low_neg'\n",
    "        elif prediction_sid == 0:\n",
    "            prediction_sid_range = 'neutral'\n",
    "        elif prediction_sid < high_pos_threshold and prediction_sid > 0:\n",
    "            prediction_sid_range = 'low_pos'\n",
    "        elif prediction_sid >= high_pos_threshold:\n",
    "            prediction_sid_range = 'high_pos'\n",
    "\n",
    "        prediction_labels.append(prediction_sid_range)\n",
    "   \n",
    "    df[\"prediction_emo\"] = prediction_labels\n",
    "\n",
    "    df.to_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t')\n",
    "\n",
    "    \n",
    "def labelEmo(model_name, eval_emo_filter, df):\n",
    "    target_labels = [] \n",
    "    prediction_labels = []\n",
    "    prediction_labels_ge = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        t_text = (row.target_text[:512] + '..') if len(row.target_text) > 512 else row.target_text\n",
    "        target_emo = goemotions(t_text)\n",
    "        target_label = Top_Score_Label(target_emo)\n",
    "        target_labels.append(target_label)\n",
    "        \n",
    "        p_text = (row.predictions[:512] + '..') if len(row.predictions) > 512 else row.predictions\n",
    "\n",
    "        prediction_emo = goemotions(p_text)\n",
    "\n",
    "        prediction_label = Top_Score_Label(prediction_emo)\n",
    "\n",
    "        prediction_emo_range = 'neutral'\n",
    "        if prediction_label in high_neg_emo:\n",
    "            prediction_emo_range = 'high_neg'\n",
    "        elif prediction_label in low_neg_emo:\n",
    "            prediction_emo_range = 'low_neg'\n",
    "        elif prediction_label in neutral_emo:\n",
    "            prediction_emo_range = 'neutral'\n",
    "        elif prediction_label in low_pos_emo:\n",
    "            prediction_emo_range = 'low_pos'\n",
    "        elif prediction_label in high_pos_emo:\n",
    "            prediction_emo_range = 'high_pos'\n",
    "\n",
    "        prediction_labels_ge.append(prediction_label)\n",
    "        prediction_labels.append(prediction_emo_range)\n",
    "\n",
    "    df[\"target_emo_ge\"] = target_labels\n",
    "    df[\"prediction_emo_ge\"] = prediction_labels_ge\n",
    "    df[\"prediction_emo\"] = prediction_labels\n",
    "\n",
    "    df.to_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t')\n",
    "\n",
    "    \n",
    "def labelGoEmo(model_name, eval_emo_filter, df):\n",
    "    target_labels = [] \n",
    "    prediction_labels = []\n",
    "    prediction_labels_ge = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        target_label = row.target_emo\n",
    "        if target_label in high_neg_emo:\n",
    "            target_emo_range = 'high_neg'\n",
    "        elif target_label in low_neg_emo:\n",
    "            target_emo_range = 'low_neg'\n",
    "        elif target_label in neutral_emo:\n",
    "            target_emo_range = 'neutral'\n",
    "        elif target_label in low_pos_emo:\n",
    "            target_emo_range = 'low_pos'\n",
    "        elif target_label in high_pos_emo:\n",
    "            target_emo_range = 'high_pos'\n",
    "\n",
    "        target_labels.append(target_emo_range)\n",
    "\n",
    "        p_text = (row.predictions[:512] + '..') if len(row.predictions) > 512 else row.predictions\n",
    "\n",
    "        prediction_emo = goemotions(p_text)\n",
    "\n",
    "        prediction_label = Top_Score_Label(prediction_emo)\n",
    "        \n",
    "        if prediction_label in high_neg_emo:\n",
    "            prediction_emo_range = 'high_neg'\n",
    "        elif prediction_label in low_neg_emo:\n",
    "            prediction_emo_range = 'low_neg'\n",
    "        elif prediction_label in neutral_emo:\n",
    "            prediction_emo_range = 'neutral'\n",
    "        elif prediction_label in low_pos_emo:\n",
    "            prediction_emo_range = 'low_pos'\n",
    "        elif prediction_label in high_pos_emo:\n",
    "            prediction_emo_range = 'high_pos'\n",
    "\n",
    "        prediction_labels_ge.append(prediction_label)\n",
    "        prediction_labels.append(prediction_emo_range)\n",
    "\n",
    "    df = df.rename(columns={\"target_emo\": \"target_emo_ge\"})\n",
    "    df[\"target_emo\"] = target_labels\n",
    "    df[\"prediction_emo_ge\"] = prediction_labels_ge\n",
    "    df[\"prediction_emo\"] = prediction_labels\n",
    "\n",
    "    df.to_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t')\n",
    "\n",
    "    \n",
    "def labelPredictions(model_emo_filter, eval_emo_filter):\n",
    "    if (model_emo_filter == 'bart'): \n",
    "        model_name = 'facebook/bart-base'       \n",
    "    else:\n",
    "        model_name = f\"{training_type}-{model_dataset}-{model_emo_filter}-{epochs}epochs-bart\"\n",
    "    print(f'---- Labeling Predictions for Model: {model_name} on Dataset: {testing_set}-{eval_emo_filter} ----')\n",
    "\n",
    "    df = pd.read_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-predictions.tsv', sep='\\t').astype(str)       \n",
    "        \n",
    "    if model_emo_filter == 'sid':\n",
    "        labelSid(model_name, eval_emo_filter, df)\n",
    "    elif model_emo_filter == 'sid_rg':\n",
    "        labelSidRg(model_name, eval_emo_filter, df)\n",
    "    elif '_ge' in model_emo_filter or '_ge' in eval_emo_filter or model_emo_filter == 'nil' or eval_emo_filter == 'nil': \n",
    "        labelGoEmo(model_name, eval_emo_filter, df)\n",
    "    else:\n",
    "        labelEmo(model_name, eval_emo_filter, df)\n",
    "        \n",
    "for emo_filter in emo_filter_list:\n",
    "    if (emo_filter == 'bart'):\n",
    "        labelPredictions('bart', 'emo')\n",
    "        labelPredictions('bart', 'emo_ge')     \n",
    "    else:\n",
    "        labelPredictions(emo_filter, emo_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ade75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import evaluate\n",
    "\n",
    "from pprint import pprint\n",
    "from statistics import mean\n",
    "\n",
    "# Exact Match scores emotion transition\n",
    "def exact(truths, preds):\n",
    "    exact = evaluate.load('exact_match')\n",
    "    result = exact.compute(predictions = preds, references = truths)['exact_match']\n",
    "    return result\n",
    "\n",
    "# BLEU, Google_BLEU, ROUGE, and METEOR score paraphrasing\n",
    "def bleu(truths, preds):\n",
    "    bleu = evaluate.load('bleu')\n",
    "    result = bleu.compute(predictions = preds, references = truths)['bleu']\n",
    "    return result\n",
    "\n",
    "def google_bleu(truths, preds):\n",
    "    google_bleu = evaluate.load('google_bleu')\n",
    "    result = google_bleu.compute(predictions = preds, references = truths)['google_bleu']\n",
    "    return result\n",
    "\n",
    "def rouge1(truths, preds):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    result = rouge.compute(predictions = preds, references = truths)['rouge1']\n",
    "    return result\n",
    "    \n",
    "def rouge2(truths, preds):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    result = rouge.compute(predictions = preds, references = truths)['rouge2']\n",
    "    return result\n",
    "    \n",
    "def rougeL(truths, preds):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    result = rouge.compute(predictions = preds, references = truths)['rougeL']\n",
    "    return result\n",
    "\n",
    "def bertscore(truths, preds):\n",
    "    bscore = evaluate.load('bertscore')\n",
    "    result = bscore.compute(predictions = preds, references = truths, model_type=\"distilbert-base-uncased\")\n",
    "    return result\n",
    "\n",
    "def meteor(truths, preds):\n",
    "    meteor = evaluate.load('meteor')\n",
    "    result = meteor.compute(predictions = preds, references = truths)['meteor']\n",
    "    return result\n",
    "\n",
    "\n",
    "def scoreSid(model_emo_filter, eval_emo_filter):\n",
    "    if (model_emo_filter == 'T5'): \n",
    "        model_name = 't5-base'\n",
    "    else:\n",
    "        model_name = f\"{training_type}-{model_dataset}-{model_emo_filter}-{epochs}epochs\"\n",
    "    print_base_info(model_name, testing_set, eval_emo_filter)\n",
    "    df = pd.read_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t').astype(str)\n",
    "    \n",
    "    row_count = 0\n",
    "    lowered_intensity = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        row_count += 1\n",
    "        if float(row.input_emo) > float(row.prediction_emo):\n",
    "            lowered_intensity += 1\n",
    "    \n",
    "    pprint(\"Exact Score\")\n",
    "    pprint(lowered_intensity/row_count)\n",
    "\n",
    "    pprint(\"BLEU Score\")\n",
    "    pprint(bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('Google BLEU Score')\n",
    "    pprint(google_bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('ROUGE1 Score')\n",
    "    pprint(rouge1(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('ROUGE2 Score')\n",
    "    pprint(rouge2(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('ROUGEL Score')\n",
    "    pprint(rougeL(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('METEOR Score')\n",
    "    pprint(meteor(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "def scorePredictions(model_emo_filter, eval_emo_filter):\n",
    "    if (model_emo_filter == 'bart'): \n",
    "        model_name = 'facebook/bart-base'\n",
    "    else:\n",
    "        model_name = f\"{training_type}-{model_dataset}-{model_emo_filter}-{epochs}epochs\"\n",
    "    print_base_info(model_name, testing_set, eval_emo_filter)\n",
    "    df = pd.read_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t').astype(str)\n",
    "\n",
    "    scores = {'Model Name & Test Set': model_name + ' & ' + f'{testing_set}-{eval_emo_filter}'}\n",
    "#    scores.update({'Test Set': f'{testing_set}-{eval_emo_filter}'})\n",
    "\n",
    "#    pprint(\"Exact Score\")\n",
    "#    pprint(exact(df[\"target_emo\"], df[\"prediction_emo\"]))\n",
    "    scores.update({'Exact': exact(df[\"target_emo\"], df[\"prediction_emo\"])})\n",
    "\n",
    "#    if ('target_emo_ge' in df.columns) and ('prediction_emo_ge' in df.columns): \n",
    "#        pprint(\"Exact Score GE\")\n",
    "#        pprint(exact(df[\"target_emo_ge\"], df[\"prediction_emo_ge\"]))\n",
    "    scores.update({'Exact GE': exact(df[\"target_emo_ge\"], df[\"prediction_emo_ge\"])})\n",
    "\n",
    "#    pprint(\"BLEU Score\")\n",
    "#    pprint(bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "    scores.update({'BLEU': bleu(df[\"target_text\"], df[\"predictions\"])})\n",
    "\n",
    "#    pprint('Google BLEU Score')\n",
    "#    pprint(google_bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "    scores.update({'Google BLEU': google_bleu(df[\"target_text\"], df[\"predictions\"])})\n",
    "\n",
    "#    pprint('ROUGE1 Score')\n",
    "#    pprint(rouge1(df[\"target_text\"], df[\"predictions\"]))\n",
    "    scores.update({'ROUGE1': rouge1(df[\"target_text\"], df[\"predictions\"])})\n",
    "\n",
    "#    pprint('ROUGE2 Score')\n",
    "#    pprint(rouge2(df[\"target_text\"], df[\"predictions\"]))\n",
    "    scores.update({'ROUGE2': rouge2(df[\"target_text\"], df[\"predictions\"])})\n",
    "\n",
    "#    pprint('ROUGEL Score')\n",
    "#    pprint(rougeL(df[\"target_text\"], df[\"predictions\"]))\n",
    "    scores.update({'ROUGEL': rougeL(df[\"target_text\"], df[\"predictions\"])})\n",
    "\n",
    "#    pprint('METEOR Score')\n",
    "#    pprint(meteor(df[\"target_text\"], df[\"predictions\"]))\n",
    "    scores.update({'METEOR': meteor(df[\"target_text\"], df[\"predictions\"])})\n",
    "\n",
    "    for key, value in scores.items(): \n",
    "        print(key + ': ', str(value))\n",
    "    print('\\n')\n",
    "\n",
    "    return scores\n",
    "\n",
    "scores_df = pd.DataFrame()\n",
    "\n",
    "for emo_filter in emo_filter_list:\n",
    "    if emo_filter == 'sid':\n",
    "        scoreSid(emo_filter, emo_filter)\n",
    "    else:\n",
    "        if (emo_filter == 'bart'):\n",
    "            scores = scorePredictions('bart', 'emo')\n",
    "            scores_df = scores_df.append(scores, ignore_index=True)\n",
    "            scores = scorePredictions('bart', 'emo_ge')\n",
    "            scores_df = scores_df.append(scores, ignore_index=True)\n",
    "        else: \n",
    "            scores = scorePredictions(emo_filter, emo_filter)\n",
    "            scores_df = scores_df.append(scores, ignore_index=True)\n",
    "\n",
    "print(scores_df)\n",
    "scores_df.to_csv(f'predictions-data/{model_dataset}-{training_type}-{epochs}epochs-all-scores.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0538ba4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d79ca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels the target and predicted texts for scoring by emotion transition and paraphrasing metrics with GPT2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Same Sigmoid Function for determining the emotion of a text\n",
    "threshold = 0.5\n",
    "def Top_Score_Label (outputs):\n",
    "    scores = 1 / (1 + np.exp(-outputs))  # Sigmoid\n",
    "    top_score = 0\n",
    "    top_label = \"\"\n",
    "    for item in scores:\n",
    "        for idx, s in enumerate(item):\n",
    "            if s > threshold:\n",
    "                if s > top_score: \n",
    "                    top_label = model.config.id2label[idx]\n",
    "    return top_label\n",
    "\n",
    "def labelSid(model_name, eval_emo_filter, df):\n",
    "    prediction_labels = []\n",
    "    for index, row in df.iterrows():\n",
    "        prediction_sid = sid.polarity_scores(rowcalculate_emos_trans.predictions)['compound']\n",
    "\n",
    "        prediction_labels.append(prediction_sid)\n",
    "   \n",
    "    df[\"prediction_emo\"] = prediction_labels\n",
    "\n",
    "    df.to_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t')\n",
    "    \n",
    "\n",
    "def labelSidRg(model_name, eval_emo_filter, df):\n",
    "    prediction_labels = []\n",
    "    for index, row in df.iterrows():\n",
    "        prediction_sid = sid.polarity_scores(row.predictions)['compound']\n",
    "\n",
    "        if prediction_sid <= high_neg_threshold:\n",
    "            prediction_sid_range = 'high_neg'\n",
    "        elif prediction_sid > high_neg_threshold and prediction_sid < 0:\n",
    "            prediction_sid_range = 'low_neg'\n",
    "        elif prediction_sid == 0:\n",
    "            prediction_sid_range = 'neutral'\n",
    "        elif prediction_sid < high_pos_threshold and prediction_sid > 0:\n",
    "            prediction_sid_range = 'low_pos'\n",
    "        elif prediction_sid >= high_pos_threshold:\n",
    "            prediction_sid_range = 'high_pos'\n",
    "\n",
    "        prediction_labels.append(prediction_sid_range)\n",
    "   \n",
    "    df[\"prediction_emo\"] = prediction_labels\n",
    "\n",
    "    df.to_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t')\n",
    "\n",
    "    \n",
    "def labelEmo(model_name, eval_emo_filter, df):\n",
    "    target_labels = [] \n",
    "    prediction_labels = []\n",
    "    prediction_labels_ge = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        t_text = (row.target_text[:512] + '..') if len(row.target_text) > 512 else row.target_text\n",
    "        target_emo = goemotions(t_text)\n",
    "        target_label = Top_Score_Label(target_emo)\n",
    "        target_labels.append(target_label)\n",
    "        \n",
    "        p_text = (row.predictions[:512] + '..') if len(row.predictions) > 512 else row.predictions\n",
    "\n",
    "        prediction_emo = goemotions(p_text)\n",
    "\n",
    "        prediction_label = Top_Score_Label(prediction_emo)\n",
    "\n",
    "        prediction_emo_range = 'neutral'\n",
    "        if prediction_label in high_neg_emo:\n",
    "            prediction_emo_range = 'high_neg'\n",
    "        elif prediction_label in low_neg_emo:\n",
    "            prediction_emo_range = 'low_neg'\n",
    "        elif prediction_label in neutral_emo:\n",
    "            prediction_emo_range = 'neutral'\n",
    "        elif prediction_label in low_pos_emo:\n",
    "            prediction_emo_range = 'low_pos'\n",
    "        elif prediction_label in high_pos_emo:\n",
    "            prediction_emo_range = 'high_pos'\n",
    "\n",
    "        prediction_labels_ge.append(prediction_label)\n",
    "        prediction_labels.append(prediction_emo_range)\n",
    "\n",
    "    df[\"target_emo_ge\"] = target_labels\n",
    "    df[\"prediction_emo_ge\"] = prediction_labels_ge\n",
    "    df[\"prediction_emo\"] = prediction_labels\n",
    "\n",
    "    df.to_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t')\n",
    "\n",
    "    \n",
    "def labelGoEmo(model_name, eval_emo_filter, df):\n",
    "    target_labels = [] \n",
    "    prediction_labels = []\n",
    "    prediction_labels_ge = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        target_label = row.target_emo\n",
    "        if target_label in high_neg_emo:\n",
    "            target_emo_range = 'high_neg'\n",
    "        elif target_label in low_neg_emo:\n",
    "            target_emo_range = 'low_neg'\n",
    "        elif target_label in neutral_emo:\n",
    "            target_emo_range = 'neutral'\n",
    "        elif target_label in low_pos_emo:\n",
    "            target_emo_range = 'low_pos'\n",
    "        elif target_label in high_pos_emo:\n",
    "            target_emo_range = 'high_pos'\n",
    "\n",
    "        target_labels.append(target_emo_range)\n",
    "\n",
    "        p_text = (row.predictions[:512] + '..') if len(row.predictions) > 512 else row.predictions\n",
    "\n",
    "        prediction_emo = goemotions(p_text)\n",
    "\n",
    "        prediction_label = Top_Score_Label(prediction_emo)\n",
    "        \n",
    "        prediction_emo_range = 'neutral'\n",
    "        if prediction_label in high_neg_emo:\n",
    "            prediction_emo_range = 'high_neg'\n",
    "        elif prediction_label in low_neg_emo:\n",
    "            prediction_emo_range = 'low_neg'\n",
    "        elif prediction_label in neutral_emo:\n",
    "            prediction_emo_range = 'neutral'\n",
    "        elif prediction_label in low_pos_emo:\n",
    "            prediction_emo_range = 'low_pos'\n",
    "        elif prediction_label in high_pos_emo:\n",
    "            prediction_emo_range = 'high_pos'\n",
    "\n",
    "        prediction_labels_ge.append(prediction_label)\n",
    "        prediction_labels.append(prediction_emo_range)\n",
    "\n",
    "    df = df.rename(columns={\"target_emo\": \"target_emo_ge\"})\n",
    "    df[\"target_emo\"] = target_labels\n",
    "    df[\"prediction_emo_ge\"] = prediction_labels_ge\n",
    "    df[\"prediction_emo\"] = prediction_labels\n",
    "\n",
    "    df.to_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t')\n",
    "\n",
    "    \n",
    "def labelPredictions(model_emo_filter, eval_emo_filter):\n",
    "    if (model_emo_filter == 'gpt'): \n",
    "        model_name = 'gpt'       \n",
    "    else:\n",
    "        model_name = f\"{training_type}-{model_dataset}-{model_emo_filter}-{epochs}epochs-gpt\"\n",
    "    print(f'---- Labeling Predictions for Model: {model_name} on Dataset: {testing_set}-{eval_emo_filter} ----')\n",
    "\n",
    "    df = pd.read_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-predictions.tsv', sep='\\t').astype(str)       \n",
    "        \n",
    "    if model_emo_filter == 'sid':\n",
    "        labelSid(model_name, eval_emo_filter, df)\n",
    "    elif model_emo_filter == 'sid_rg':\n",
    "        labelSidRg(model_name, eval_emo_filter, df)\n",
    "    elif '_ge' in model_emo_filter or '_ge' in eval_emo_filter or model_emo_filter == 'nil' or eval_emo_filter == 'nil': \n",
    "        labelGoEmo(model_name, eval_emo_filter, df)\n",
    "    else:\n",
    "        labelEmo(model_name, eval_emo_filter, df)\n",
    "        \n",
    "for emo_filter in emo_filter_list:\n",
    "    if (emo_filter == 'gpt'):\n",
    "        labelPredictions('gpt', 'emo')\n",
    "        labelPredictions('gpt', 'emo_ge')     \n",
    "    else:\n",
    "        labelPredictions(emo_filter, emo_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f808566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import evaluate\n",
    "\n",
    "from pprint import pprint\n",
    "from statistics import mean\n",
    "\n",
    "# Exact Match scores emotion transition\n",
    "def exact(truths, preds):\n",
    "    exact = evaluate.load('exact_match')\n",
    "    result = exact.compute(predictions = preds, references = truths)['exact_match']\n",
    "    return result\n",
    "\n",
    "# BLEU, Google_BLEU, ROUGE, and METEOR score paraphrasing\n",
    "def bleu(truths, preds):\n",
    "    bleu = evaluate.load('bleu')\n",
    "    result = bleu.compute(predictions = preds, references = truths)['bleu']\n",
    "    return result\n",
    "\n",
    "def google_bleu(truths, preds):\n",
    "    google_bleu = evaluate.load('google_bleu')\n",
    "    result = google_bleu.compute(predictions = preds, references = truths)['google_bleu']\n",
    "    return result\n",
    "\n",
    "def rouge1(truths, preds):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    result = rouge.compute(predictions = preds, references = truths)['rouge1']\n",
    "    return result\n",
    "    \n",
    "def rouge2(truths, preds):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    result = rouge.compute(predictions = preds, references = truths)['rouge2']\n",
    "    return result\n",
    "    \n",
    "def rougeL(truths, preds):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    result = rouge.compute(predictions = preds, references = truths)['rougeL']\n",
    "    return result\n",
    "\n",
    "def bertscore(truths, preds):\n",
    "    bscore = evaluate.load('bertscore')\n",
    "    result = bscore.compute(predictions = preds, references = truths, model_type=\"distilbert-base-uncased\")\n",
    "    return result\n",
    "\n",
    "def meteor(truths, preds):\n",
    "    meteor = evaluate.load('meteor')\n",
    "    result = meteor.compute(predictions = preds, references = truths)['meteor']\n",
    "    return result\n",
    "\n",
    "\n",
    "def scoreSid(model_emo_filter, eval_emo_filter):\n",
    "    if (model_emo_filter == 'T5'): \n",
    "        model_name = 't5-base'\n",
    "    else:\n",
    "        model_name = f\"{training_type}-{model_dataset}-{model_emo_filter}-{epochs}epochs\"\n",
    "    print_base_info(model_name, testing_set, eval_emo_filter)\n",
    "    df = pd.read_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t').astype(str)\n",
    "    \n",
    "    row_count = 0\n",
    "    lowered_intensity = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        row_count += 1\n",
    "        if float(row.input_emo) > float(row.prediction_emo):\n",
    "            lowered_intensity += 1\n",
    "    \n",
    "    pprint(\"Exact Score\")\n",
    "    pprint(lowered_intensity/row_count)\n",
    "\n",
    "    pprint(\"BLEU Score\")\n",
    "    pprint(bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('Google BLEU Score')\n",
    "    pprint(google_bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('ROUGE1 Score')\n",
    "    pprint(rouge1(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('ROUGE2 Score')\n",
    "    pprint(rouge2(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('ROUGEL Score')\n",
    "    pprint(rougeL(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('METEOR Score')\n",
    "    pprint(meteor(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "def scorePredictions(model_emo_filter, eval_emo_filter):\n",
    "    if (model_emo_filter == 'gpt'): \n",
    "        model_name = 'gpt'\n",
    "    else:\n",
    "        model_name = f\"{training_type}-{model_dataset}-{model_emo_filter}-{epochs}epochs-gpt\"\n",
    "    print_base_info(model_name, testing_set, eval_emo_filter)\n",
    "    df = pd.read_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t').astype(str)\n",
    "\n",
    "    scores = {'Model Name & Test Set': model_name + ' & ' + f'{testing_set}-{eval_emo_filter}'}\n",
    "#    scores.update({'Test Set': f'{testing_set}-{eval_emo_filter}'})\n",
    "\n",
    "#    pprint(\"Exact Score\")\n",
    "#    pprint(exact(df[\"target_emo\"], df[\"prediction_emo\"]))\n",
    "    scores.update({'Exact': exact(df[\"target_emo\"], df[\"prediction_emo\"])})\n",
    "\n",
    "#    if ('target_emo_ge' in df.columns) and ('prediction_emo_ge' in df.columns): \n",
    "#        pprint(\"Exact Score GE\")\n",
    "#        pprint(exact(df[\"target_emo_ge\"], df[\"prediction_emo_ge\"]))\n",
    "    scores.update({'Exact GE': exact(df[\"target_emo_ge\"], df[\"prediction_emo_ge\"])})\n",
    "\n",
    "#    pprint(\"BLEU Score\")\n",
    "#    pprint(bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "    scores.update({'BLEU': bleu(df[\"target_text\"], df[\"predictions\"])})\n",
    "\n",
    "#    pprint('Google BLEU Score')\n",
    "#    pprint(google_bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "    scores.update({'Google BLEU': google_bleu(df[\"target_text\"], df[\"predictions\"])})\n",
    "\n",
    "#    pprint('ROUGE1 Score')\n",
    "#    pprint(rouge1(df[\"target_text\"], df[\"predictions\"]))\n",
    "    scores.update({'ROUGE1': rouge1(df[\"target_text\"], df[\"predictions\"])})\n",
    "\n",
    "#    pprint('ROUGE2 Score')\n",
    "#    pprint(rouge2(df[\"target_text\"], df[\"predictions\"]))\n",
    "    scores.update({'ROUGE2': rouge2(df[\"target_text\"], df[\"predictions\"])})\n",
    "\n",
    "#    pprint('ROUGEL Score')\n",
    "#    pprint(rougeL(df[\"target_text\"], df[\"predictions\"]))\n",
    "    scores.update({'ROUGEL': rougeL(df[\"target_text\"], df[\"predictions\"])})\n",
    "\n",
    "#    pprint('METEOR Score')\n",
    "#    pprint(meteor(df[\"target_text\"], df[\"predictions\"]))\n",
    "    scores.update({'METEOR': meteor(df[\"target_text\"], df[\"predictions\"])})\n",
    "\n",
    "    for key, value in scores.items(): \n",
    "        print(key + ': ', str(value))\n",
    "    print('\\n')\n",
    "\n",
    "    return scores\n",
    "\n",
    "scores_df = pd.DataFrame()\n",
    "\n",
    "for emo_filter in emo_filter_list:\n",
    "    if emo_filter == 'sid':\n",
    "        scoreSid(emo_filter, emo_filter)\n",
    "    else:\n",
    "        if (emo_filter == 'gpt'):\n",
    "            scores = scorePredictions('gpt', 'emo')\n",
    "            scores_df = scores_df.append(scores, ignore_index=True)\n",
    "            scores = scorePredictions('gpt', 'emo_ge')\n",
    "            scores_df = scores_df.append(scores, ignore_index=True)\n",
    "        else: \n",
    "            scores = scorePredictions(emo_filter, emo_filter)\n",
    "            scores_df = scores_df.append(scores, ignore_index=True)\n",
    "\n",
    "print(scores_df)\n",
    "scores_df.to_csv(f'predictions-data/{model_dataset}-{training_type}-{epochs}epochs-all-scores.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3d2c09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbeb6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are the old GPT2 label and score procedures \n",
    "\n",
    "# Labels the target and predicted texts for scoring by emotion transition and paraphrasing metrics with GPT2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Same Sigmoid Function for determining the emotion of a text\n",
    "threshold = 0.5\n",
    "def Top_Score_Label (outputs):\n",
    "    scores = 1 / (1 + np.exp(-outputs))  # Sigmoid\n",
    "    top_score = 0\n",
    "    top_label = \"\"\n",
    "    for item in scores:\n",
    "        for idx, s in enumerate(item):\n",
    "            if s > threshold:\n",
    "                if s > top_score: \n",
    "                    top_label = model.config.id2label[idx]\n",
    "    return top_label\n",
    "\n",
    "def labelSidGPT(model_name, eval_emo_filter, df):\n",
    "    prediction_labels = []\n",
    "    for index, row in df.iterrows():\n",
    "        prediction_sid = sid.polarity_scores(row.predictions)['compound']\n",
    "\n",
    "        prediction_labels.append(prediction_sid)\n",
    "   \n",
    "    df[\"prediction_emo\"] = prediction_labels\n",
    "\n",
    "    df.to_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t')\n",
    "    \n",
    "\n",
    "def labelSidRgGPT(model_name, eval_emo_filter, df):\n",
    "    prediction_labels = []\n",
    "    for index, row in df.iterrows():\n",
    "        prediction_sid = sid.polarity_scores(row.predictions)['compound']\n",
    "\n",
    "        if prediction_sid <= high_neg_threshold:\n",
    "            prediction_sid_range = 'high_neg'\n",
    "        elif prediction_sid > high_neg_threshold and prediction_sid < 0:\n",
    "            prediction_sid_range = 'low_neg'\n",
    "        elif prediction_sid == 0:\n",
    "            prediction_sid_range = 'neutral'\n",
    "        elif prediction_sid < high_pos_threshold and prediction_sid > 0:\n",
    "            prediction_sid_range = 'low_pos'\n",
    "        elif prediction_sid >= high_pos_threshold:\n",
    "            prediction_sid_range = 'high_pos'\n",
    "\n",
    "        prediction_labels.append(prediction_sid_range)\n",
    "   \n",
    "    df[\"prediction_emo\"] = prediction_labels\n",
    "\n",
    "    df.to_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t')\n",
    "\n",
    "    \n",
    "def labelEmoGPT(model_name, eval_emo_filter, df):\n",
    "    prediction_labels = []\n",
    "    for index, row in df.iterrows():\n",
    "        p_text = (row.predictions[:512] + '..') if len(row.predictions) > 512 else row.predictions\n",
    "\n",
    "        prediction_emo = goemotions(p_text)\n",
    "\n",
    "        prediction_label = Top_Score_Label(prediction_emo)\n",
    "\n",
    "        if prediction_label in high_neg_emo:\n",
    "            prediction_emo_range = 'high_neg'\n",
    "        elif prediction_label in low_neg_emo:\n",
    "            prediction_emo_range = 'low_neg'\n",
    "        elif prediction_label in neutral_emo:\n",
    "            prediction_emo_range = 'neutral'\n",
    "        elif prediction_label in low_pos_emo:\n",
    "            prediction_emo_range = 'low_pos'\n",
    "        elif prediction_label in high_pos_emo:\n",
    "            prediction_emo_range = 'high_pos'\n",
    "\n",
    "        prediction_labels.append(prediction_emo_range)\n",
    "   \n",
    "    df[\"prediction_emo\"] = prediction_labels\n",
    "\n",
    "    df.to_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t')\n",
    "\n",
    "    \n",
    "def labelGoEmoGPT(model_name, eval_emo_filter, df):\n",
    "    prediction_labels = []\n",
    "    for index, row in df.iterrows():\n",
    "        p_text = (row.predictions[:512] + '..') if len(row.predictions) > 512 else row.predictions\n",
    "\n",
    "        prediction_emo = goemotions(p_text)\n",
    "\n",
    "        prediction_label = Top_Score_Label(prediction_emo)\n",
    "        \n",
    "        prediction_labels.append(prediction_label)\n",
    "   \n",
    "    df[\"prediction_emo\"] = prediction_labels\n",
    "\n",
    "    df.to_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t')\n",
    "\n",
    "    \n",
    "def labelPredictionsGPT(model_emo_filter, eval_emo_filter):\n",
    "    model_name = f\"{training_type}-{model_dataset}-{model_emo_filter}-{epochs}epochs-gpt\"\n",
    "    print(f'---- Labeling Predictions for Model: {model_name} on Dataset: {testing_set}-{eval_emo_filter} ----')\n",
    "\n",
    "    df = pd.read_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-predictions.tsv', sep='\\t').astype(str) \n",
    "    \n",
    "    if model_emo_filter == 'sid':\n",
    "        labelSidGPT(model_name, eval_emo_filter, df)\n",
    "    elif model_emo_filter == 'sid_rg':\n",
    "        labelSidRgGPT(model_name, eval_emo_filter, df)\n",
    "    elif '_ge' in model_emo_filter: \n",
    "        labelGoEmoGPT(model_name, eval_emo_filter, df)\n",
    "    else:\n",
    "        labelEmoGPT(model_name, eval_emo_filter, df)\n",
    "        \n",
    "for emo_filter in emo_filter_list:\n",
    "    if (emo_filter == 'gpt'):\n",
    "        labelPredictionsGPT('gpt', 'emo')\n",
    "    else: \n",
    "        labelPredictionsGPT(emo_filter, emo_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55532e54",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import evaluate\n",
    "\n",
    "from pprint import pprint\n",
    "from statistics import mean\n",
    "\n",
    "# Exact Match scores emotion transition\n",
    "def exact(truths, preds):\n",
    "    exact = evaluate.load('exact_match')\n",
    "    result = exact.compute(predictions = preds, references = truths)['exact_match']\n",
    "    return result\n",
    "\n",
    "# BLEU, Google_BLEU, ROUGE, and METEOR score paraphrasing\n",
    "def bleu(truths, preds):\n",
    "    bleu = evaluate.load('bleu')\n",
    "    result = bleu.compute(predictions = preds, references = truths)['bleu']\n",
    "    return result\n",
    "\n",
    "def google_bleu(truths, preds):\n",
    "    google_bleu = evaluate.load('google_bleu')\n",
    "    result = google_bleu.compute(predictions = preds, references = truths)['google_bleu']\n",
    "    return result\n",
    "\n",
    "def rouge1(truths, preds):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    result = rouge.compute(predictions = preds, references = truths)['rouge1']\n",
    "    return result\n",
    "    \n",
    "def rouge2(truths, preds):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    result = rouge.compute(predictions = preds, references = truths)['rouge2']\n",
    "    return result\n",
    "    \n",
    "def rougeL(truths, preds):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    result = rouge.compute(predictions = preds, references = truths)['rougeL']\n",
    "    return result\n",
    "\n",
    "def bertscore(truths, preds):\n",
    "    bscore = evaluate.load('bertscore')\n",
    "    result = bscore.compute(predictions = preds, references = truths, model_type=\"distilbert-base-uncased\")\n",
    "    return result\n",
    "\n",
    "def meteor(truths, preds):\n",
    "    meteor = evaluate.load('meteor')\n",
    "    result = meteor.compute(predictions = preds, references = truths)['meteor']\n",
    "    return result\n",
    "\n",
    "\n",
    "def scoreSidGPT(model_emo_filter, eval_emo_filter):\n",
    "    model_name = f\"{training_type}-{model_dataset}-{model_emo_filter}-{epochs}epochs-gpt\"\n",
    "    print_base_info(model_name, testing_set, eval_emo_filter)\n",
    "    df = pd.read_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t').astype(str)\n",
    "    \n",
    "    row_count = 0\n",
    "    lowered_intensity = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        row_count += 1\n",
    "        if float(row.input_emo) > float(row.prediction_emo):\n",
    "            lowered_intensity += 1\n",
    "    \n",
    "    pprint(\"Exact Score\")\n",
    "    pprint(lowered_intensity/row_count)\n",
    "\n",
    "    pprint(\"BLEU Score\")\n",
    "    pprint(bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('Google BLEU Score')\n",
    "    pprint(google_bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('ROUGE1 Score')\n",
    "    pprint(rouge1(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('ROUGE2 Score')\n",
    "    pprint(rouge2(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('ROUGEL Score')\n",
    "    pprint(rougeL(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('METEOR Score')\n",
    "    pprint(meteor(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "def scorePredictionsGPT(model_emo_filter, eval_emo_filter):\n",
    "    model_name = f\"{training_type}-{model_dataset}-{model_emo_filter}-{epochs}epochs-gpt\"\n",
    "    print_base_info(model_name, testing_set, eval_emo_filter)\n",
    "    df = pd.read_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t').astype(str)\n",
    "    \n",
    "    pprint(\"Exact Score\")\n",
    "    pprint(exact(df[\"target_emo\"], df[\"prediction_emo\"]))\n",
    "\n",
    "    pprint(\"BLEU Score\")\n",
    "    pprint(bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('Google BLEU Score')\n",
    "    pprint(google_bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('ROUGE1 Score')\n",
    "    pprint(rouge1(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('ROUGE2 Score')\n",
    "    pprint(rouge2(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('ROUGEL Score')\n",
    "    pprint(rougeL(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('METEOR Score')\n",
    "    pprint(meteor(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    print('\\n')\n",
    "    \n",
    "def orgScoreSidGPT(model_emo_filter, eval_emo_filter):\n",
    "    model_name = f\"{training_type}-{model_dataset}-{model_emo_filter}-{epochs}epochs-gpt\"\n",
    "    print_base_info(model_name, testing_set, eval_emo_filter)\n",
    "    df = pd.read_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t').astype(str)\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    row_count = 0\n",
    "    lowered_intensity = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        row_count += 1\n",
    "        if float(row.input_emo) > float(row.prediction_emo):\n",
    "            lowered_intensity += 1\n",
    "    \n",
    "    outputs.append(lowered_intensity/row_count)\n",
    "    outputs.append(bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "    outputs.append(google_bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "    outputs.append(rouge1(df[\"target_text\"], df[\"predictions\"]))\n",
    "    outputs.append(rouge2(df[\"target_text\"], df[\"predictions\"]))\n",
    "    outputs.append(rougeL(df[\"target_text\"], df[\"predictions\"]))\n",
    "    outputs.append(meteor(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    for metric in outputs:\n",
    "        pprint(metric)\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "def orgScorePredictionsGPT(model_emo_filter, eval_emo_filter):\n",
    "    model_name = f\"{training_type}-{model_dataset}-{model_emo_filter}-{epochs}epochs-gpt\"\n",
    "    print_base_info(model_name, testing_set, eval_emo_filter)\n",
    "    df = pd.read_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t').astype(str)\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    outputs.append(exact(df[\"target_emo\"], df[\"prediction_emo\"]))\n",
    "    outputs.append(bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "    outputs.append(google_bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "    outputs.append(rouge1(df[\"target_text\"], df[\"predictions\"]))\n",
    "    outputs.append(rouge2(df[\"target_text\"], df[\"predictions\"]))\n",
    "    outputs.append(rougeL(df[\"target_text\"], df[\"predictions\"]))\n",
    "    outputs.append(meteor(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    for metric in outputs:\n",
    "        pprint(metric)\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "for emo_filter in emo_filter_list:\n",
    "    if emo_filter == 'sid':\n",
    "        orgScoreSidGPT(emo_filter, emo_filter)\n",
    "    else:\n",
    "        if (emo_filter == 'gpt'):\n",
    "            orgScorePredictionsGPT('gpt', 'emo')\n",
    "        else: \n",
    "            orgScorePredictionsGPT(emo_filter, emo_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c72ffce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
