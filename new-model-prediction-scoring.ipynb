{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6002bda5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers==2.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec099ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a39611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import os\n",
    "from transformers import BertTokenizer\n",
    "from GoEmotions.model import BertForMultiLabelClassification\n",
    "from GoEmotions.multilabel_pipeline import MultiLabelPipeline\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"monologg/bert-base-cased-goemotions-original\")\n",
    "model = BertForMultiLabelClassification.from_pretrained(\"monologg/bert-base-cased-goemotions-original\")\n",
    "\n",
    "goemotions = MultiLabelPipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    threshold=0.3,\n",
    "    device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43224a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dataset = input('Dataset to train model (mix, twit0.825 or combined):  ')\n",
    "training_type = input('Regular or limited training (regular or limited): ')\n",
    "epochs = int(input('Number of training epochs: '))\n",
    "\n",
    "testing_set = input('Dataset for prediction generation (mix, twit0.825, or combined): ')\n",
    "\n",
    "if training_type.lower() == 'regular':\n",
    "    train_set = 'training'\n",
    "    eval_set = 'testing'\n",
    "elif training_type.lower() == 'limited':\n",
    "    train_set = 'testing'\n",
    "    eval_set = 'training'\n",
    "else:\n",
    "    print('Please enter a valid training type')\n",
    "\n",
    "def print_base_info(model_name, testing_set, eval_emo_filter):\n",
    "    print(f'---- Scoring Predictions ----')\n",
    "    print(f'Model: {model_name}')\n",
    "    print(f'Test Set: {testing_set}-{eval_emo_filter}')\n",
    "    \n",
    "emo_filter_list = [\n",
    "                   'sid', \n",
    "                   'sid_rg', \n",
    "                   'emo', \n",
    "                   'emo_sid', \n",
    "                   'emo_sid_tg', \n",
    "                   'emo_sid_tg_nn', \n",
    "                   'emo_sid_tg_ge', \n",
    "                   'emo_sid_tg_nn_ge'\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f169ba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_neg_emo = {'anger', 'disgust', 'grief', 'fear', 'sadness'}\n",
    "low_neg_emo = {'nervousness', 'annoyance', 'disappointment', 'embarrassment', 'remorse', 'disapproval'}\n",
    "neutral_emo = {'confusion', 'curiosity', 'realization', 'surprise', 'neutral'}\n",
    "low_pos_emo = {'approval', 'caring', 'desire', 'relief'}\n",
    "high_pos_emo = {'amusement', 'excitement', 'pride', 'optimism', 'gratitude', 'joy', 'admiration', 'love'}\n",
    "\n",
    "high_neg_threshold = -0.4\n",
    "high_pos_threshold = 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae76e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels the target and predicted texts for scoring by emotion transition and paraphrasing metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Same Sigmoid Function for determining the emotion of a text\n",
    "threshold = 0.5\n",
    "def Top_Score_Label (outputs):\n",
    "    scores = 1 / (1 + np.exp(-outputs))  # Sigmoid\n",
    "    top_score = 0\n",
    "    top_label = \"\"\n",
    "    for item in scores:\n",
    "        for idx, s in enumerate(item):\n",
    "            if s > threshold:\n",
    "                if s > top_score: \n",
    "                    top_label = model.config.id2label[idx]\n",
    "    return top_label\n",
    "\n",
    "def labelSid(model_name, eval_emo_filter, df):\n",
    "    prediction_labels = []\n",
    "    for index, row in df.iterrows():\n",
    "        prediction_sid = sid.polarity_scores(row.predictions)['compound']\n",
    "\n",
    "        prediction_labels.append(prediction_sid)\n",
    "   \n",
    "    df[\"prediction_emo\"] = prediction_labels\n",
    "\n",
    "    df.to_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t')\n",
    "    \n",
    "\n",
    "def labelSidRg(model_name, eval_emo_filter, df):\n",
    "    prediction_labels = []\n",
    "    for index, row in df.iterrows():\n",
    "        prediction_sid = sid.polarity_scores(row.predictions)['compound']\n",
    "\n",
    "        if prediction_sid <= high_neg_threshold:\n",
    "            prediction_sid_range = 'high_neg'\n",
    "        elif prediction_sid > high_neg_threshold and prediction_sid < 0:\n",
    "            prediction_sid_range = 'low_neg'\n",
    "        elif prediction_sid == 0:\n",
    "            prediction_sid_range = 'neutral'\n",
    "        elif prediction_sid < high_pos_threshold and prediction_sid > 0:\n",
    "            prediction_sid_range = 'low_pos'\n",
    "        elif prediction_sid >= high_pos_threshold:\n",
    "            prediction_sid_range = 'high_pos'\n",
    "\n",
    "        prediction_labels.append(prediction_sid_range)\n",
    "   \n",
    "    df[\"prediction_emo\"] = prediction_labels\n",
    "\n",
    "    df.to_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t')\n",
    "\n",
    "    \n",
    "def labelEmo(model_name, eval_emo_filter, df):\n",
    "    prediction_labels = []\n",
    "    for index, row in df.iterrows():\n",
    "        p_text = (row.predictions[:512] + '..') if len(row.predictions) > 512 else row.predictions\n",
    "\n",
    "        prediction_emo = goemotions(p_text)\n",
    "\n",
    "        prediction_label = Top_Score_Label(prediction_emo)\n",
    "\n",
    "        if prediction_label in high_neg_emo:\n",
    "            prediction_emo_range = 'high_neg'\n",
    "        elif prediction_label in low_neg_emo:\n",
    "            prediction_emo_range = 'low_neg'\n",
    "        elif prediction_label in neutral_emo:\n",
    "            prediction_emo_range = 'neutral'\n",
    "        elif prediction_label in low_pos_emo:\n",
    "            prediction_emo_range = 'low_pos'\n",
    "        elif prediction_label in high_pos_emo:\n",
    "            prediction_emo_range = 'high_pos'\n",
    "\n",
    "        prediction_labels.append(prediction_emo_range)\n",
    "   \n",
    "    df[\"prediction_emo\"] = prediction_labels\n",
    "\n",
    "    df.to_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t')\n",
    "\n",
    "    \n",
    "def labelGoEmo(model_name, eval_emo_filter, df):\n",
    "    prediction_labels = []\n",
    "    for index, row in df.iterrows():\n",
    "        p_text = (row.predictions[:512] + '..') if len(row.predictions) > 512 else row.predictions\n",
    "\n",
    "        prediction_emo = goemotions(p_text)\n",
    "\n",
    "        prediction_label = Top_Score_Label(prediction_emo)\n",
    "        \n",
    "        prediction_labels.append(prediction_label)\n",
    "   \n",
    "    df[\"prediction_emo\"] = prediction_labels\n",
    "\n",
    "    df.to_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t')\n",
    "\n",
    "    \n",
    "def labelPredictions(model_emo_filter, eval_emo_filter):\n",
    "    model_name = f\"{training_type}-{model_dataset}-{model_emo_filter}-{epochs}epochs\"\n",
    "    print(f'---- Labeling Predictions for Model: {model_name} on Dataset: {testing_set}-{eval_emo_filter} ----')\n",
    "\n",
    "    df = pd.read_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-predictions.tsv', sep='\\t').astype(str) \n",
    "    \n",
    "    if model_emo_filter == 'sid':\n",
    "        labelSid(model_name, eval_emo_filter, df)\n",
    "    elif model_emo_filter == 'sid_rg':\n",
    "        labelSidRg(model_name, eval_emo_filter, df)\n",
    "    elif '_ge' in model_emo_filter: \n",
    "        labelGoEmo(model_name, eval_emo_filter, df)\n",
    "    else:\n",
    "        labelEmo(model_name, eval_emo_filter, df)\n",
    "        \n",
    "for emo_filter in emo_filter_list:\n",
    "    labelPredictions(emo_filter, emo_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75568c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import evaluate\n",
    "\n",
    "from pprint import pprint\n",
    "from statistics import mean\n",
    "\n",
    "# Exact Match scores emotion transition\n",
    "def exact(truths, preds):\n",
    "    exact = evaluate.load('exact_match')\n",
    "    result = exact.compute(predictions = preds, references = truths)['exact_match']\n",
    "    return result\n",
    "\n",
    "# BLEU, Google_BLEU, ROUGE, and METEOR score paraphrasing\n",
    "def bleu(truths, preds):\n",
    "    bleu = evaluate.load('bleu')\n",
    "    result = bleu.compute(predictions = preds, references = truths)['bleu']\n",
    "    return result\n",
    "\n",
    "def google_bleu(truths, preds):\n",
    "    google_bleu = evaluate.load('google_bleu')\n",
    "    result = google_bleu.compute(predictions = preds, references = truths)['google_bleu']\n",
    "    return result\n",
    "\n",
    "def rouge1(truths, preds):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    result = rouge.compute(predictions = preds, references = truths)['rouge1']\n",
    "    return result\n",
    "    \n",
    "def rouge2(truths, preds):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    result = rouge.compute(predictions = preds, references = truths)['rouge2']\n",
    "    return result\n",
    "    \n",
    "def rougeL(truths, preds):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    result = rouge.compute(predictions = preds, references = truths)['rougeL']\n",
    "    return result\n",
    "\n",
    "def bertscore(truths, preds):\n",
    "    bscore = evaluate.load('bertscore')\n",
    "    result = bscore.compute(predictions = preds, references = truths, model_type=\"distilbert-base-uncased\")\n",
    "    return result\n",
    "\n",
    "def meteor(truths, preds):\n",
    "    meteor = evaluate.load('meteor')\n",
    "    result = meteor.compute(predictions = preds, references = truths)['meteor']\n",
    "    return result\n",
    "\n",
    "\n",
    "def scoreSid(model_emo_filter, eval_emo_filter):\n",
    "    model_name = f\"{training_type}-{model_dataset}-{model_emo_filter}-{epochs}epochs\"\n",
    "    print_base_info(model_name, testing_set, eval_emo_filter)\n",
    "    df = pd.read_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t').astype(str)\n",
    "    \n",
    "    row_count = 0\n",
    "    lowered_intensity = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        row_count += 1\n",
    "        if float(row.input_emo) > float(row.prediction_emo):\n",
    "            lowered_intensity += 1\n",
    "    \n",
    "    pprint(\"Exact Score\")\n",
    "    pprint(lowered_intensity/row_count)\n",
    "\n",
    "    pprint(\"BLEU Score\")\n",
    "    pprint(bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('Google BLEU Score')\n",
    "    pprint(google_bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('ROUGE1 Score')\n",
    "    pprint(rouge1(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('ROUGE2 Score')\n",
    "    pprint(rouge2(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('ROUGEL Score')\n",
    "    pprint(rougeL(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('METEOR Score')\n",
    "    pprint(meteor(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "def scorePredictions(model_emo_filter, eval_emo_filter):\n",
    "    model_name = f\"{training_type}-{model_dataset}-{model_emo_filter}-{epochs}epochs\"\n",
    "    print_base_info(model_name, testing_set, eval_emo_filter)\n",
    "    df = pd.read_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t').astype(str)\n",
    "    \n",
    "    pprint(\"Exact Score\")\n",
    "    pprint(exact(df[\"target_emo\"], df[\"prediction_emo\"]))\n",
    "\n",
    "    pprint(\"BLEU Score\")\n",
    "    pprint(bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('Google BLEU Score')\n",
    "    pprint(google_bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('ROUGE1 Score')\n",
    "    pprint(rouge1(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('ROUGE2 Score')\n",
    "    pprint(rouge2(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('ROUGEL Score')\n",
    "    pprint(rougeL(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('METEOR Score')\n",
    "    pprint(meteor(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    print('\\n')\n",
    "    \n",
    "for emo_filter in emo_filter_list:\n",
    "    if emo_filter == 'sid':\n",
    "        scoreSid(emo_filter, emo_filter)\n",
    "    else:\n",
    "        scorePredictions(emo_filter, emo_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3d2c09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbeb6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels the target and predicted texts for scoring by emotion transition and paraphrasing metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Same Sigmoid Function for determining the emotion of a text\n",
    "threshold = 0.5\n",
    "def Top_Score_Label (outputs):\n",
    "    scores = 1 / (1 + np.exp(-outputs))  # Sigmoid\n",
    "    top_score = 0\n",
    "    top_label = \"\"\n",
    "    for item in scores:\n",
    "        for idx, s in enumerate(item):\n",
    "            if s > threshold:\n",
    "                if s > top_score: \n",
    "                    top_label = model.config.id2label[idx]\n",
    "    return top_label\n",
    "\n",
    "def labelSidGPT(model_name, eval_emo_filter, df):\n",
    "    prediction_labels = []\n",
    "    for index, row in df.iterrows():\n",
    "        prediction_sid = sid.polarity_scores(row.predictions)['compound']\n",
    "\n",
    "        prediction_labels.append(prediction_sid)\n",
    "   \n",
    "    df[\"prediction_emo\"] = prediction_labels\n",
    "\n",
    "    df.to_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t')\n",
    "    \n",
    "\n",
    "def labelSidRgGPT(model_name, eval_emo_filter, df):\n",
    "    prediction_labels = []\n",
    "    for index, row in df.iterrows():\n",
    "        prediction_sid = sid.polarity_scores(row.predictions)['compound']\n",
    "\n",
    "        if prediction_sid <= high_neg_threshold:\n",
    "            prediction_sid_range = 'high_neg'\n",
    "        elif prediction_sid > high_neg_threshold and prediction_sid < 0:\n",
    "            prediction_sid_range = 'low_neg'\n",
    "        elif prediction_sid == 0:\n",
    "            prediction_sid_range = 'neutral'\n",
    "        elif prediction_sid < high_pos_threshold and prediction_sid > 0:\n",
    "            prediction_sid_range = 'low_pos'\n",
    "        elif prediction_sid >= high_pos_threshold:\n",
    "            prediction_sid_range = 'high_pos'\n",
    "\n",
    "        prediction_labels.append(prediction_sid_range)\n",
    "   \n",
    "    df[\"prediction_emo\"] = prediction_labels\n",
    "\n",
    "    df.to_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t')\n",
    "\n",
    "    \n",
    "def labelEmoGPT(model_name, eval_emo_filter, df):\n",
    "    prediction_labels = []\n",
    "    for index, row in df.iterrows():\n",
    "        p_text = (row.predictions[:512] + '..') if len(row.predictions) > 512 else row.predictions\n",
    "\n",
    "        prediction_emo = goemotions(p_text)\n",
    "\n",
    "        prediction_label = Top_Score_Label(prediction_emo)\n",
    "\n",
    "        if prediction_label in high_neg_emo:\n",
    "            prediction_emo_range = 'high_neg'\n",
    "        elif prediction_label in low_neg_emo:\n",
    "            prediction_emo_range = 'low_neg'\n",
    "        elif prediction_label in neutral_emo:\n",
    "            prediction_emo_range = 'neutral'\n",
    "        elif prediction_label in low_pos_emo:\n",
    "            prediction_emo_range = 'low_pos'\n",
    "        elif prediction_label in high_pos_emo:\n",
    "            prediction_emo_range = 'high_pos'\n",
    "\n",
    "        prediction_labels.append(prediction_emo_range)\n",
    "   \n",
    "    df[\"prediction_emo\"] = prediction_labels\n",
    "\n",
    "    df.to_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t')\n",
    "\n",
    "    \n",
    "def labelGoEmoGPT(model_name, eval_emo_filter, df):\n",
    "    prediction_labels = []\n",
    "    for index, row in df.iterrows():\n",
    "        p_text = (row.predictions[:512] + '..') if len(row.predictions) > 512 else row.predictions\n",
    "\n",
    "        prediction_emo = goemotions(p_text)\n",
    "\n",
    "        prediction_label = Top_Score_Label(prediction_emo)\n",
    "        \n",
    "        prediction_labels.append(prediction_label)\n",
    "   \n",
    "    df[\"prediction_emo\"] = prediction_labels\n",
    "\n",
    "    df.to_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t')\n",
    "\n",
    "    \n",
    "def labelPredictionsGPT(model_emo_filter, eval_emo_filter):\n",
    "    model_name = f\"{training_type}-{model_dataset}-{model_emo_filter}-{epochs}epochs-gpt\"\n",
    "    print(f'---- Labeling Predictions for Model: {model_name} on Dataset: {testing_set}-{eval_emo_filter} ----')\n",
    "\n",
    "    df = pd.read_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-predictions.tsv', sep='\\t').astype(str) \n",
    "    \n",
    "    if model_emo_filter == 'sid':\n",
    "        labelSidGPT(model_name, eval_emo_filter, df)\n",
    "    elif model_emo_filter == 'sid_rg':\n",
    "        labelSidRgGPT(model_name, eval_emo_filter, df)\n",
    "    elif '_ge' in model_emo_filter: \n",
    "        labelGoEmoGPT(model_name, eval_emo_filter, df)\n",
    "    else:\n",
    "        labelEmoGPT(model_name, eval_emo_filter, df)\n",
    "        \n",
    "for emo_filter in emo_filter_list:\n",
    "    labelPredictionsGPT(emo_filter, emo_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55532e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import evaluate\n",
    "\n",
    "from pprint import pprint\n",
    "from statistics import mean\n",
    "\n",
    "# Exact Match scores emotion transition\n",
    "def exact(truths, preds):\n",
    "    exact = evaluate.load('exact_match')\n",
    "    result = exact.compute(predictions = preds, references = truths)['exact_match']\n",
    "    return result\n",
    "\n",
    "# BLEU, Google_BLEU, ROUGE, and METEOR score paraphrasing\n",
    "def bleu(truths, preds):\n",
    "    bleu = evaluate.load('bleu')\n",
    "    result = bleu.compute(predictions = preds, references = truths)['bleu']\n",
    "    return result\n",
    "\n",
    "def google_bleu(truths, preds):\n",
    "    google_bleu = evaluate.load('google_bleu')\n",
    "    result = google_bleu.compute(predictions = preds, references = truths)['google_bleu']\n",
    "    return result\n",
    "\n",
    "def rouge1(truths, preds):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    result = rouge.compute(predictions = preds, references = truths)['rouge1']\n",
    "    return result\n",
    "    \n",
    "def rouge2(truths, preds):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    result = rouge.compute(predictions = preds, references = truths)['rouge2']\n",
    "    return result\n",
    "    \n",
    "def rougeL(truths, preds):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    result = rouge.compute(predictions = preds, references = truths)['rougeL']\n",
    "    return result\n",
    "\n",
    "def bertscore(truths, preds):\n",
    "    bscore = evaluate.load('bertscore')\n",
    "    result = bscore.compute(predictions = preds, references = truths, model_type=\"distilbert-base-uncased\")\n",
    "    return result\n",
    "\n",
    "def meteor(truths, preds):\n",
    "    meteor = evaluate.load('meteor')\n",
    "    result = meteor.compute(predictions = preds, references = truths)['meteor']\n",
    "    return result\n",
    "\n",
    "\n",
    "def scoreSidGPT(model_emo_filter, eval_emo_filter):\n",
    "    model_name = f\"{training_type}-{model_dataset}-{model_emo_filter}-{epochs}epochs-gpt\"\n",
    "    print_base_info(model_name, testing_set, eval_emo_filter)\n",
    "    df = pd.read_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t').astype(str)\n",
    "    \n",
    "    row_count = 0\n",
    "    lowered_intensity = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        row_count += 1\n",
    "        if float(row.input_emo) > float(row.prediction_emo):\n",
    "            lowered_intensity += 1\n",
    "    \n",
    "    pprint(\"Exact Score\")\n",
    "    pprint(lowered_intensity/row_count)\n",
    "\n",
    "    pprint(\"BLEU Score\")\n",
    "    pprint(bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('Google BLEU Score')\n",
    "    pprint(google_bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('ROUGE1 Score')\n",
    "    pprint(rouge1(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('ROUGE2 Score')\n",
    "    pprint(rouge2(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('ROUGEL Score')\n",
    "    pprint(rougeL(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('METEOR Score')\n",
    "    pprint(meteor(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "def scorePredictionsGPT(model_emo_filter, eval_emo_filter):\n",
    "    model_name = f\"{training_type}-{model_dataset}-{model_emo_filter}-{epochs}epochs-gpt\"\n",
    "    print_base_info(model_name, testing_set, eval_emo_filter)\n",
    "    df = pd.read_csv(f'predictions-data/{model_name}/{testing_set}-{eval_emo_filter}-prediction_emo.tsv', sep='\\t').astype(str)\n",
    "    \n",
    "    pprint(\"Exact Score\")\n",
    "    pprint(exact(df[\"target_emo\"], df[\"prediction_emo\"]))\n",
    "\n",
    "    pprint(\"BLEU Score\")\n",
    "    pprint(bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('Google BLEU Score')\n",
    "    pprint(google_bleu(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('ROUGE1 Score')\n",
    "    pprint(rouge1(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('ROUGE2 Score')\n",
    "    pprint(rouge2(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('ROUGEL Score')\n",
    "    pprint(rougeL(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    pprint('METEOR Score')\n",
    "    pprint(meteor(df[\"target_text\"], df[\"predictions\"]))\n",
    "\n",
    "    print('\\n')\n",
    "    \n",
    "for emo_filter in emo_filter_list:\n",
    "    if emo_filter == 'sid':\n",
    "        scoreSidGPT(emo_filter, emo_filter)\n",
    "    else:\n",
    "        scorePredictionsGPT(emo_filter, emo_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c72ffce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
